# Configuration for training on pre-computed AIMv2 Native feature sequences

# --- Model Identification ---
# (Used for naming output files/runs)
model:
  base: "AnatomyFlaws"
  rev: "v9.1_adabelief_fl_AIMv2Native_3000" # New revision indicating feature mode
  arch: class                      # Still a classification task

# --- Data Configuration ---
data:
  mode: features                   # <<< Crucial: Set mode to 'features' >>>
  data_root: ./data                # Root location for data folders
  # <<< Name of the subfolder containing the generated .npz features >>>
  feature_dir_name: "aimv2_large_patch14_native_SeqFeatures_fp16"
  # <<< Validation split count (applied to feature file list) >>>
  val_split_count: 300

# --- Head Model Parameters ---
# (Defines the HeadModel structure in head_model.py)
head_params:
  features: 1024                   # <<< Input features from AIMv2 base (MUST MATCH features) >>>
  pooling_strategy: 'attn'         # Pooling strategy ('attn', 'avg', 'none') for HeadModel input sequences
  hidden_dim: 1024                 # Hidden dimension for the MLP head
  num_res_blocks: 3                # Number of ResBlocks in the head
  dropout_rate: 0.218              # Dropout rate within the head
  output_mode: sigmoid             # <<< Output mode for the head

# --- Attention Pooling Parameters (Only if head_params.pooling_strategy='attn') ---
attn_pool_params:
  attn_pool_heads: 16              # Heads for the Attention Pooling layer in HeadModel
  attn_pool_dropout: 0.218         # Dropout for the Attention Pooling layer

# --- Training Parameters ---
train:
  # --- Training Duration ---
  # (Adjust based on faster training speed - maybe more steps/epochs feasible now?)
  max_train_steps: 10000            # Example: 5k steps (adjust as needed)
  # max_train_epochs: 20           # Can alternatively specify epochs

  # --- Core Settings ---
  # (Can likely use a higher LR now since only training head)
  lr: 1e-4                         # <<< Increased LR from E2E example >>>
  batch: 32                        # <<< Increase batch size significantly - head is small! >>>
  precision: "bf16"                # Mixed precision for training head

  # --- Loss Function ---
  loss_function: "focal"
  focal_loss_gamma: 2.0

  # --- Optimizer ---
  optimizer: "adabelief" # ADOPTAOScheduleFree
  # -- Optimizer Hyperparameters --
  betas: [0.9, 0.95]
  eps: 1e-8
  weight_decay: 1e-3               # Regularization might still be useful for head
  rectify: true
  weight_decouple: true

  gamma: 0.005             # Gamma for FMARSCrop (float)
  r_sf: 0.0                # R value for ScheduleFree (float)
  wlpow_sf: 2.0            # Weight LR Power for ScheduleFree (float)
  state_precision: parameter # Precision for optimizer state in ScheduleFree (parameter or float32)
  adaptive_clip: 1.0       # Adaptive clipping threshold for ADOPT/FMARS/ScheduleFree variants
  adaptive_clip_eps: 1e-3  # Epsilon for adaptive clipping

  # --- Scheduler ---
  scheduler_name: "RexAnnealingWarmRestarts"
  # -- Scheduler Arguments --
  scheduler_gamma: 0.95
  scheduler_cycle_multiplier: 1.0
  # <<< Adjust first cycle steps to match max_train_steps >>>
  scheduler_first_cycle_max_steps: 3000
  scheduler_min_lr: 1e-7
  # <<< Adjust warmup steps relative to new steps/epoch (batch 64) >>>
  # If Steps/Epoch ≈ (2400 samples / 64 batch) ≈ 38 steps... maybe 50-100 warmup steps?
  scheduler_warmup_steps: 500

  # --- Data & Other ---
  seed: 218
  num_workers: 8                   # <<< Can use more workers now for loading NPZ >>>
  # preload_data: false            # Not applicable/needed for features mode
  nsave: 1000                      # Periodic save frequency
  log_every_n: 1                  # Log training metrics frequency
  validate_every_n: 50            # Validation frequency (adjust based on steps/epoch)
  save_full_model: false           # <<< Save ONLY HeadModel (head+pooler) >>>

# --- Labels ---
# (Same as before)
labels:
 '0': { name: "Bad Anatomy", loss: 1.0 }
 '1': { name: "Good Anatomy", loss: 1.0 }

# --- Logging ---
wandb_project: "city-classifiers" # Or maybe "city-classifiers-features"
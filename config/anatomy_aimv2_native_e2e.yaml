# config/anatomy_aimv2_native_e2e.yaml
# Configuration for End-to-End Anatomy Flaw classification using AIMv2 Native

# --- Model Identification & Type ---
model:
  base: "AnatomyFlaws"
  rev: "v8.0_adabelief_fl_AIMv2Native_3000" # Revision for filenames
  arch: class
  is_end_to_end: true                         # <<< Specify End-to-End Mode >>>
  base_vision_model: "apple/aimv2-large-patch14-native" # <<< Base model for E2E >>>

  # --- Early Feature Extraction / Base Model Settings ---
  extract_layer: -1                           # Use last hidden state (default)
  pooling_strategy: 'attn'                    # Pooling: 'cls', 'avg', 'attn', 'pooler'
  freeze_base_model: true                     # Freeze the base vision model weights?

  # --- Attention Pooling Settings (Only used if pooling_strategy='attn') ---
  attn_pool_heads: 16                         # Heads for the Attention Pooling layer
  attn_pool_dropout: 0.218                    # Dropout for the Attention Pooling layer

# --- Head Parameters (for EarlyExtractAnatomyModel's head) ---
head_params:
  hidden_dim: 1024                            # Hidden dimension for the classification head
  num_res_blocks: 3                           # Number of ResBlocks in the head
  dropout_rate: 0.218                         # Dropout rate within the head's down block
  output_mode: sigmoid                        # 'linear', 'sigmoid', 'softmax', 'tanh_scaled'

# --- Training Parameters ---
train:
  # --- Training Duration ---
  max_train_steps: 1000   # Max total steps (batches processed)
  # max_train_epochs:

  # --- Core Settings ---
  lr: 1e-4                                    # Learning rate (Low for frozen base or fine-tuning)
  batch: 4                                   # Batch size per device
  precision: "bf16"                           # Training precision: "fp32", "fp16", "bf16"

  # --- Loss Function ---
  loss_function: "focal"                      # Loss: 'crossentropy', 'focal', 'bce', 'l1', 'mse', 'nll', 'ghm'
  focal_loss_gamma: 2.0                       # Gamma for Focal Loss

  # --- Optimizer ---
  optimizer: "adabelief"                      # Optimizer name (lowercase, from optimizer/ or 'adamw')
  # -- Optimizer Hyperparameter Overrides (Optional) --
  betas: [0.9, 0.95]
  eps: 1e-8
  weight_decay: 1e-3
  rectify: true
  weight_decouple: true
  # clip_threshold: 1.0 # Example for CAME

  # --- Scheduler ---
  scheduler_name: "RexAnnealingWarmRestarts"  # Scheduler name (lowercase, from optimizer/ or standard torch, or None)
  # -- Scheduler Argument Overrides (Optional, prefix custom with 'scheduler_') --
  scheduler_gamma: 0.95
  scheduler_cycle_multiplier: 1.0
  scheduler_first_cycle_max_steps: 300      # <<< Match max_train_steps for single cycle >>>
  scheduler_min_lr: 1e-7
  scheduler_warmup_steps: 100                 # Warmup steps (used by some schedulers)
  r_sf: 0.0 # Example for ScheduleFree
  wlpow_sf: 2.0 # Example for ScheduleFree
  state_precision: parameter # Example for ScheduleFree qfp8

  # --- Data & Other ---
  val_split_count: 100                       # Samples PER CLASS for validation
  seed: 218
  num_workers: 0                             # Dataloader workers
  preload_data: false                        # Should be false for E2E image loading
  nsave: 500                                 # Save periodic checkpoint frequency (steps)
  save_full_model: true                      # Save full state dict along with base model if in e2e
# --- Data Root ---
# Parent folder containing the raw image class folders (e.g., anatomy/0/, anatomy/1/)
data_root: "anatomy"

# --- Logging ---
wandb_project: "city-classifiers"             # WandB project name

# --- Labels ---
# Define classes. Keys must match folder names. 'name' is for display, 'loss' is optional weight.
labels:
 '0': { name: "Bad Anatomy", loss: 1.0 }
 '1': { name: "Good Anatomy", loss: 1.0 }